{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mergekit' already exists and is not an empty directory.\n",
      "mllm/fine_tune/imdb_dataset/mergekit/mergekit\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /system/linux/mambaforge-3.11/lib/python3.11/site-packages/matlabengineforpython-9.14-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file://mllm/fine_tune/imdb_dataset/mergekit/mergekit\n",
      "\u001b[31mERROR: file://mllm/fine_tune/imdb_dataset/mergekit/mergekit does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/cg123/mergekit.git\n",
    "%cd mergekit\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"./finetune_gpt2_merged_base\"  # folder to store the result in\n",
    "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
    "CONFIG_YML = \"mllm/test_merge/fine_tune_merged.yml\"  # merge configuration file\n",
    "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
    "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10908.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 819/819 [00:05<00:00, 157.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# actually do merge\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    options=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        copy_tokenizer=COPY_TOKENIZER,\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "        allow_crimes=True\n",
    "    ),\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /system/linux/mambaforge-3.11/lib/python3.11/site-packages/matlabengineforpython-9.14-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import ModelCard, ModelCardData\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = HF_USERNAME
    "\n",
    "template_text = \"\"\"\n",
    "---\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- merge\n",
    "- mergekit\n",
    "- lazymergekit\n",
    "{%- for model in models %}\n",
    "- {{ model }}\n",
    "{%- endfor %}\n",
    "---\n",
    "\n",
    "# {{ model_name }}\n",
    "\n",
    "{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n",
    "\n",
    "{%- for model in models %}\n",
    "* [{{ model }}](https://huggingface.co/{{ model }})\n",
    "{%- endfor %}\n",
    "\n",
    "## ðŸ§© Configuration\n",
    "\n",
    "\\```yaml\n",
    "{{- yaml_config -}}\n",
    "\\```\n",
    "\"\"\"\n",
    "yaml_config = \"\"\"\n",
    "models:\n",
    "  - model: imdb_finetune_epoch_1_gpt2\n",
    "    parameters:\n",
    "      weight: 1.0\n",
    "\n",
    "  - model: imdb_finetune_epoch_5_gpt2\n",
    "    parameters:\n",
    "      weight: 1.0\n",
    "\n",
    "merge_method: linear\n",
    "dtype: float16\n",
    "\"\"\"\n",
    "# Create a Jinja template object\n",
    "jinja_template = Template(template_text.strip())\n",
    "data = yaml.safe_load(yaml_config)\n",
    "MODEL_NAME = \"Merged_imdb\"\n",
    "if \"models\" in data:\n",
    "    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\n",
    "elif \"parameters\" in data:\n",
    "    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\n",
    "elif \"slices\" in data:\n",
    "    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\n",
    "else:\n",
    "    raise Exception(\"No models or slices found in yaml config\")\n",
    "\n",
    "# Fill the template\n",
    "content = jinja_template.render(\n",
    "    model_name=MODEL_NAME,\n",
    "    models=models,\n",
    "    yaml_config=yaml_config,\n",
    "    username=username,\n",
    ")\n",
    "\n",
    "# Save the model card\n",
    "card = ModelCard(content)\n",
    "card.save('merge/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f705fe0ede4115ac4f086a128e7863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00001.safetensors:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Merged_imdb/commit/20412ddc6267ba0cefd8720cc6c57c74c59e1d21', commit_message='Upload folder using huggingface_hub', commit_description='', oid='20412ddc6267ba0cefd8720cc6c57c74c59e1d21', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = HF_USERNAME
    "\n",
    "# Defined in the secrets tab in Google Colab\n",
    "api = HfApi(token=\"HF_TOKEN\")\n",
    "\n",
    "api.create_repo(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "api.upload_folder(\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"finetune_gpt2_merged_base\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
