{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /system/linux/mambaforge-3.11/lib/python3.11/site-packages/matlabengineforpython-9.14-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in .local/lib/python3.11/site-packages (4.37.2)\n",
      "Requirement already satisfied: datasets in .local/lib/python3.11/site-packages (2.17.1)\n",
      "Requirement already satisfied: evaluate in .local/lib/python3.11/site-packages (0.4.1)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in .local/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in .local/lib/python3.11/site-packages (from transformers) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .local/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in .local/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in .local/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in .local/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in .local/lib/python3.11/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in .local/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: xxhash in .local/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in .local/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in .local/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: click in .local/lib/python3.11/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in .local/lib/python3.11/site-packages (from nltk->rouge_score) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/linux/mambaforge-3.11/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=5490276cf743e6f0c6021942ee4fb6cd96eb5f934a15b8e8499b52de8b5fb246\n",
      "  Stored in directory: /tmp/.xdg_cache_phudish_p/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461e30299d774395bf90caf0447efd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "billsum = billsum.train_test_split(test_size=0.2,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".local/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc + \" TL;DR\" for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True,padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=1024, truncation=True,padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f306443ecbf84b718a9c0f939d7f4459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3950d191980d485dad62d8595b472f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nThis act shall be known, and may be cited, as the Pool Safety Act.\\nSEC. 2.\\nThe Legislature finds and declares all of the following:\\n(a) Swimming pools provide children and their families with a wonderful opportunity for recreation, exercise, and fun. Keeping children safe during this activity is supported by parents and guardians, safety advocates, health providers, insurance companies, and the swimming pool industry.\\n(b) According to both the federal Centers for Disease Control and Prevention’s National Center for Injury Prevention and Control and the State Department of Public Health’s EpiCenter data, drowning is the leading cause of death for California children one to four years of age, inclusive.\\n(c) Additional children suffer near-drowning incidents and survive, but many of those children suffer irreversible brain injuries, which can lead to lifelong learning deficiencies that impact not only the affected child and his or her family, but also the resources and moneys available to California’s health care system, regional centers, and special education school programs.\\n(d) Close parental supervision of children with access to swimming pools is essential to providing pool safety for children. Barriers, such as those required pursuant to Section 115922 of the Health and Safety Code, can help to deter young children from gaining unsupervised access to pools. Swimming lessons are encouraged and can help children understand the importance of water safety.\\n(e) All water sports activities come with risk. Knowing the risks and having drowning prevention strategies in place before and during water sports activities reduce drowning incidents, and the installation of a residential pool barrier is a leading strategy to further California’s goal of dramatically reducing unintentional injury.\\nSEC. 3.\\nSection 7195 of the Business and Professions Code is amended to read:\\n7195.\\nFor purposes of this chapter, the following definitions apply:\\n(a) (1) “Home inspection” is a noninvasive, physical examination, performed for a fee in connection with a transfer, as defined in subdivision (e), of real property, of the mechanical, electrical, or plumbing systems or the structural and essential components of a residential dwelling of one to four units designed to identify material defects in those systems, structures, and components. “Home inspection” includes any consultation regarding the property that is represented to be a home inspection or any confusingly similar term.\\n(2) In connection with a transfer, as defined in subdivision (e), of real property with a swimming pool or spa, a “home inspection” shall include a noninvasive physical examination of the pool or spa and dwelling for the purpose of identifying which, if any, of the seven drowning prevention safety features listed in subdivision (a) of Section 115922 of the Health and Safety Code the pool or spa is equipped with.\\n(3) “Home inspection,” if requested by the client, may include an inspection of energy efficiency. Energy efficiency items to be inspected may include the following:\\n(A) A noninvasive inspection of insulation R-values in attics, roofs, walls, floors, and ducts.\\n(B) The number of window glass panes and frame types.\\n(C) The heating and cooling equipment and water heating systems.\\n(D) The age and fuel type of major appliances.\\n(E) The exhaust and cooling fans.\\n(F) The type of thermostat and other systems.\\n(G) The general integrity and potential leakage areas of walls, window areas, doors, and duct systems.\\n(H) The solar control efficiency of existing windows.\\n(b) A “material defect” is a condition that significantly affects the value, desirability, habitability, or safety of the dwelling. Style or aesthetics shall not be considered in determining whether a system, structure, or component is defective.\\n(c) A “home inspection report” is a written report prepared for a fee and issued after a home inspection. The report clearly describes and identifies the inspected systems, structures, or components of the dwelling, any material defects identified, and any recommendations regarding the conditions observed or recommendations for evaluation by appropriate persons. In a dwelling with a pool or spa, the “home inspection report” shall identify which, if any, of the seven drowning prevention safety features listed in subdivision (a) of Section 115922 of the Health and Safety Code the pool or spa is equipped with and shall specifically state if the pool or spa has fewer than two of the listed drowning prevention safety features.\\n(d) A “home inspector” is any individual who performs a home inspection.\\n(e) “Transfer” is a transfer by sale, exchange, installment land sales contract, as defined in Section 2985 of the Civil Code, lease with an option to purchase, any other option to purchase, or ground lease coupled with improvements, of real property or residential stock cooperative, improved with or consisting of not less than one nor more than four dwelling units.\\nSEC. 4.\\nSection 115922 of the Health and Safety Code is amended to read:\\n115922.\\n(a) Except as provided in Section 115925, when a building permit is issued for the construction of a new swimming pool or spa or the remodeling of an existing swimming pool or spa at a private single-family home, the swimming pool or spa shall be equipped with at least two of the following seven drowning prevention safety features:\\n(1) An enclosure that meets the requirements of Section 115923 and isolates the swimming pool or spa from the private single-family home.\\n(2) Removable mesh fencing that meets American Society for Testing and Materials (ASTM) Specifications F 2286 standards in conjunction with a gate that is self-closing and self-latching and can accommodate a key lockable device.\\n(3) An approved safety pool cover, as defined in subdivision (d) of Section 115921.\\n(4) Exit alarms on the private single-family home’s doors that provide direct access to the swimming pool or spa. The exit alarm may cause either an alarm noise or a verbal warning, such as a repeating notification that “the door to the pool is open.”\\n(5) A self-closing, self-latching device with a release mechanism placed no lower than 54 inches above the floor on the private single-family home’s doors providing direct access to the swimming pool or spa.\\n(6) An alarm that, when placed in a swimming pool or spa, will sound upon detection of accidental or unauthorized entrance into the water. The alarm shall meet and be independently certified to the ASTM Standard F 2208 “Standards Specification for Pool Alarms,” which includes surface motion, pressure, sonar, laser, and infrared type alarms. A swimming protection alarm feature designed for individual use, including an alarm attached to a child that sounds when the child exceeds a certain distance or becomes submerged in water, is not a qualifying drowning prevention safety feature.\\n(7) Other means of protection, if the degree of protection afforded is equal to or greater than that afforded by any of the features set forth above and has been independently verified by an approved testing laboratory as meeting standards for those features established by the ASTM or the American Society of Mechanical Engineers (ASME).\\n(b) Before the issuance of a final approval for the completion of permitted construction or remodeling work, the local building code official shall inspect the drowning safety prevention features required by this act and, if no violations are found, shall give final approval.\\nSEC. 5.\\nSection 115925 of the Health and Safety Code is amended to read:\\n115925.\\nThe requirements of this article shall not apply to any of the following:\\n(a) Public swimming pools.\\n(b) Hot tubs or spas with locking safety covers that comply with the American Society for Testing Materials (ASTM F1346).\\n(c) An apartment complex, or any residential setting other than a single-family home.\\nSEC. 6.\\nIf the Commission on State Mandates determines that this act contains costs mandated by the state, reimbursement to local agencies and school districts for those costs shall be made pursuant to Part 7 (commencing with Section 17500) of Division 4 of Title 2 of the Government Code.', 'summary': 'Existing law, the Swimming Pool Safety Act, provides that it does not apply to any pool within the jurisdiction of any political subdivision that adopts an ordinance for swimming pools, as specified. The act further requires, when a building permit is issued for construction of a new swimming pool or spa, or the remodeling of an existing pool or spa, at a private, single-family home, that the pool or spa be equipped with at least 1 of 7 drowning prevention safety features. The act requires the local building code official to inspect and approve the drowning safety prevention devices before the issuance of a final approval for the completion of permitted construction or remodeling work.\\nThis bill would instead require, when a building permit is issued, that the pool or spa be equipped with at least 2 of the 7 drowning prevention safety features. By imposing additional duties on local officials, this bill would impose a state-mandated local program. The bill would remove the exemption for the above-described political subdivisions.\\nExisting law defines terms related to paid home inspections, establishes a standard of care for home inspectors, and prohibits certain inspections in which the inspector or the inspector’s employer, as specified, has a financial interest.\\nThis bill would require a home inspection for real property with a swimming pool or spa to include a noninvasive physical examination of the pool or spa and dwelling for the purpose of identifying which, if any, of the specified 7 drowning prevention safety features the pool or spa is equipped with.\\nThe California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\\nThis bill would provide that, if the Commission on State Mandates determines that the bill contains costs mandated by the state, reimbursement for those costs shall be made pursuant to these statutory provisions.', 'title': 'An act to amend Section 7195 of the Business and Professions Code, and to amend Sections 115922 and 115925 of the Health and Safety Code, relating to public health.', 'input_ids': [16345, 3876, 1096, 25, 383, 661, 286, 262, 1812, 286, 3442, 466, 10865, 355, 5679, 25, 628, 198, 50, 24565, 352, 13, 198, 1212, 719, 2236, 307, 1900, 11, 290, 743, 307, 9181, 11, 355, 262, 19850, 11233, 2191, 13, 198, 23683, 13, 362, 13, 198, 464, 20815, 7228, 290, 24183, 477, 286, 262, 1708, 25, 198, 7, 64, 8, 2451, 27428, 20354, 2148, 1751, 290, 511, 4172, 351, 257, 7932, 3663, 329, 27702, 11, 5517, 11, 290, 1257, 13, 33311, 1751, 3338, 1141, 428, 3842, 318, 4855, 416, 3397, 290, 32769, 11, 3747, 11009, 11, 1535, 9549, 11, 5096, 2706, 11, 290, 262, 14899, 5933, 2831, 13, 198, 7, 65, 8, 4784, 284, 1111, 262, 2717, 22223, 329, 17344, 6779, 290, 18313, 447, 247, 82, 2351, 3337, 329, 40883, 18313, 290, 6779, 290, 262, 1812, 2732, 286, 5094, 3893, 447, 247, 82, 4551, 72, 23656, 1366, 11, 32249, 318, 262, 3756, 2728, 286, 1918, 329, 3442, 1751, 530, 284, 1440, 812, 286, 2479, 11, 19889, 13, 198, 7, 66, 8, 15891, 1751, 8659, 1474, 12, 67, 2053, 278, 10207, 290, 7866, 11, 475, 867, 286, 883, 1751, 8659, 43635, 3632, 6821, 11, 543, 460, 1085, 284, 25837, 4673, 33589, 326, 2928, 407, 691, 262, 5676, 1200, 290, 465, 393, 607, 1641, 11, 475, 635, 262, 4133, 290, 285, 505, 893, 1695, 284, 3442, 447, 247, 82, 1535, 1337, 1080, 11, 7915, 10399, 11, 290, 2041, 3707, 1524, 4056, 13, 198, 7, 67, 8, 13872, 21694, 20865, 286, 1751, 351, 1895, 284, 14899, 20354, 318, 6393, 284, 4955, 5933, 3747, 329, 1751, 13, 2409, 8910, 11, 884, 355, 883, 2672, 12997, 284, 7275, 12279, 24, 1828, 286, 262, 3893, 290, 11233, 6127, 11, 460, 1037, 284, 2206, 1862, 1751, 422, 13977, 555, 16668, 16149, 1895, 284, 20354, 13, 2451, 27428, 11658, 389, 10085, 290, 460, 1037, 1751, 1833, 262, 6817, 286, 1660, 3747, 13, 198, 7, 68, 8, 1439, 1660, 5701, 4568, 1282, 351, 2526, 13, 29154, 262, 7476, 290, 1719, 32249, 14196, 10064, 287, 1295, 878, 290, 1141, 1660, 5701, 4568, 4646, 32249, 10207, 11, 290, 262, 9988, 286, 257, 12420, 5933, 13054, 318, 257, 3756, 4811, 284, 2252, 3442, 447, 247, 82, 3061, 286, 12034, 8868, 48398, 5095, 13, 198, 23683, 13, 513, 13, 198, 16375, 767, 22186, 286, 262, 7320, 290, 4415, 6202, 6127, 318, 11412, 284, 1100, 25, 198, 22, 22186, 13, 198, 1890, 4959, 286, 428, 6843, 11, 262, 1708, 17336, 4174, 25, 198, 7, 64, 8, 357, 16, 8, 564, 250, 16060, 15210, 447, 251, 318, 257, 1729, 259, 23747, 11, 3518, 12452, 11, 6157, 329, 257, 6838, 287, 4637, 351, 257, 4351, 11, 355, 5447, 287, 29648, 357, 68, 828, 286, 1103, 3119, 11, 286, 262, 12370, 11, 12278, 11, 393, 40199, 3341, 393, 262, 13204, 290, 6393, 6805, 286, 257, 12420, 26765, 286, 530, 284, 1440, 4991, 3562, 284, 5911, 2587, 22448, 287, 883, 3341, 11, 8573, 11, 290, 6805, 13, 564, 250, 16060, 15210, 447, 251, 3407, 597, 18103, 5115, 262, 3119, 326, 318, 7997, 284, 307, 257, 1363, 15210, 393, 597, 15337, 306, 2092, 3381, 13, 198, 7, 17, 8, 554, 4637, 351, 257, 4351, 11, 355, 5447, 287, 29648, 357, 68, 828, 286, 1103, 3119, 351, 257, 14899, 5933, 393, 41900, 11, 257, 564, 250, 11195, 15210, 447, 251, 2236, 2291, 257, 1729, 259, 23747, 3518, 12452, 286, 262, 5933, 393, 41900, 290, 26765, 329, 262, 4007, 286, 13720, 543, 11, 611, 597, 11, 286, 262, 3598, 32249, 14196, 3747, 3033, 5610, 287, 29648, 357, 64, 8, 286, 7275, 12279, 24, 1828, 286, 262, 3893, 290, 11233, 6127, 262, 5933, 393, 41900, 318, 10911, 351, 13, 198, 7, 18, 8, 564, 250, 16060, 15210, 11, 447, 251, 611, 9167, 416, 262, 5456, 11, 743, 2291, 281, 15210, 286, 2568, 9332, 13, 6682, 9332, 3709, 284, 307, 34295, 743, 2291, 262, 1708, 25, 198, 7, 32, 8, 317, 1729, 259, 23747, 15210, 286, 32806, 371, 12, 27160, 287, 708, 873, 11, 42251, 11, 7714, 11, 18570, 11, 290, 28494, 82, 13, 198, 7, 33, 8, 383, 1271, 286, 4324, 5405, 3425, 274, 290, 5739, 3858, 13, 198, 7, 34, 8, 383, 16930, 290, 15134, 5112, 290, 1660, 16930, 3341, 13, 198, 7, 35, 8, 383, 2479, 290, 5252, 2099, 286, 1688, 29834, 13, 198, 7, 36, 8, 383, 12142, 290, 15134, 3296, 13, 198, 7, 37, 8, 383, 2099, 286, 10811, 1712, 265, 290, 584, 3341, 13, 198, 7, 38, 8, 383, 2276, 11540, 290, 2785, 47988, 3006, 286, 7714, 11, 4324, 3006, 11, 8215, 11, 290, 28494, 3341, 13, 198, 7, 39, 8, 383, 6591, 1630, 9332, 286, 4683, 9168, 13, 198, 7, 65, 8, 317, 564, 250, 33665, 11855, 447, 251, 318, 257, 4006, 326, 5566, 10975, 262, 1988, 11, 748, 343, 1799, 11, 7947, 1799, 11, 393, 3747, 286, 262, 26765, 13, 17738, 393, 35431, 2236, 407, 307, 3177, 287, 13213, 1771, 257, 1080, 11, 4645, 11, 393, 7515, 318, 32129, 13, 198, 7, 66, 8, 317, 564, 250, 11195, 15210, 989, 447, 251, 318, 257, 3194, 989, 5597, 329, 257, 6838, 290, 4884, 706, 257, 1363, 15210, 13, 383, 989, 4084, 8477, 290, 21079, 262, 34295, 3341, 11, 8573, 11, 393, 6805, 286, 262, 26765, 11, 597, 2587, 22448, 5174, 11, 290, 597, 10763, 5115, 262, 3403, 6515, 393, 10763, 329, 12660, 416, 5035, 6506, 13, 554, 257, 26765, 351, 257, 5933, 393, 41900, 11, 262, 564, 250, 11195, 15210, 989, 447, 251, 2236, 5911, 543, 11, 611, 597, 11, 286, 262, 3598, 32249, 14196, 3747, 3033, 5610, 287, 29648, 357, 64, 8, 286, 7275, 12279, 24, 1828, 286, 262, 3893, 290, 11233, 6127, 262, 5933, 393, 41900, 318, 10911, 351, 290, 2236, 5734, 1181, 611, 262, 5933, 393, 41900, 468, 7380, 621, 734, 286, 262, 5610, 32249, 14196, 3747, 3033, 13, 198, 7, 67, 8, 317, 564, 250, 11195, 24110, 447, 251, 318, 597, 1981, 508, 17706, 257, 1363, 15210, 13, 198, 7, 68, 8, 564, 250, 43260, 447, 251, 318, 257, 4351, 416, 5466, 11, 5163, 11, 25168, 1956, 4200, 2775, 11, 355, 5447, 287, 7275, 2808, 5332, 286, 262, 7511, 6127, 11, 15278, 351, 281, 3038, 284], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [3109, 9665, 1099, 11, 262, 2451, 27428, 19850, 11233, 2191, 11, 3769, 326, 340, 857, 407, 4174, 284, 597, 5933, 1626, 262, 12934, 286, 597, 1964, 29648, 326, 4344, 912, 281, 19784, 329, 14899, 20354, 11, 355, 7368, 13, 383, 719, 2252, 4433, 11, 618, 257, 2615, 8749, 318, 4884, 329, 5103, 286, 257, 649, 14899, 5933, 393, 41900, 11, 393, 262, 38977, 10809, 286, 281, 4683, 5933, 393, 41900, 11, 379, 257, 2839, 11, 2060, 12, 17989, 1363, 11, 326, 262, 5933, 393, 41900, 307, 10911, 351, 379, 1551, 352, 286, 767, 32249, 14196, 3747, 3033, 13, 383, 719, 4433, 262, 1957, 2615, 2438, 1743, 284, 10104, 290, 14762, 262, 32249, 3747, 14196, 4410, 878, 262, 30145, 286, 257, 2457, 7546, 329, 262, 11939, 286, 10431, 5103, 393, 38977, 10809, 670, 13, 198, 1212, 2855, 561, 2427, 2421, 11, 618, 257, 2615, 8749, 318, 4884, 11, 326, 262, 5933, 393, 41900, 307, 10911, 351, 379, 1551, 362, 286, 262, 767, 32249, 14196, 3747, 3033, 13, 2750, 20814, 3224, 10741, 319, 1957, 2828, 11, 428, 2855, 561, 13551, 257, 1181, 12, 22249, 515, 1957, 1430, 13, 383, 2855, 561, 4781, 262, 19572, 329, 262, 2029, 12, 34869, 1964, 45944, 3279, 13, 198, 3109, 9665, 1099, 15738, 2846, 3519, 284, 3432, 1363, 30287, 11, 30742, 257, 3210, 286, 1337, 329, 1363, 29136, 11, 290, 24059, 1728, 30287, 287, 543, 262, 24110, 393, 262, 24110, 447, 247, 82, 9749, 11, 355, 7368, 11, 468, 257, 3176, 1393, 13, 198, 1212, 2855, 561, 2421, 257, 1363, 15210, 329, 1103, 3119, 351, 257, 14899, 5933, 393, 41900, 284, 2291, 257, 1729, 259, 23747, 3518, 12452, 286, 262, 5933, 393, 41900, 290, 26765, 329, 262, 4007, 286, 13720, 543, 11, 611, 597, 11, 286, 262, 7368, 767, 32249, 14196, 3747, 3033, 262, 5933, 393, 41900, 318, 10911, 351, 13, 198, 464, 3442, 7965, 4433, 262, 1181, 284, 25903, 1957, 5942, 290, 1524, 12815, 329, 1728, 3484, 28853, 416, 262, 1181, 13, 5133, 17957, 8617, 4474, 9021, 329, 1642, 326, 37030, 13, 198, 1212, 2855, 561, 2148, 326, 11, 611, 262, 4513, 319, 1812, 13314, 689, 15947, 326, 262, 2855, 4909, 3484, 28853, 416, 262, 1181, 11, 37030, 329, 883, 3484, 2236, 307, 925, 12997, 284, 777, 20693, 8617, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_billsum[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #print(f\"Predictions : {predictions}\")\n",
    "    #print(f\"Labels : {labels}\")\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 989\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_billsum[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_billsum[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='989' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 34/989 00:02 < 01:17, 12.36 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[39m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmy_awesome_billsum_model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     push_to_hub\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1530\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1528\u001b[0m     \u001b[39m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1529\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1530\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1531\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1532\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1533\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1534\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1535\u001b[0m     )\n\u001b[1;32m   1536\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1537\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:1911\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1906\u001b[0m         nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1907\u001b[0m             amp\u001b[39m.\u001b[39mmaster_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer),\n\u001b[1;32m   1908\u001b[0m             args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1909\u001b[0m         )\n\u001b[1;32m   1910\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1911\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mclip_grad_norm_(\n\u001b[1;32m   1912\u001b[0m             model\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m   1913\u001b[0m             args\u001b[39m.\u001b[39;49mmax_grad_norm,\n\u001b[1;32m   1914\u001b[0m         )\n\u001b[1;32m   1916\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/accelerator.py:2101\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   2098\u001b[0m     \u001b[39m# `accelerator.backward(loss)` is doing that automatically. Therefore, its implementation is not needed\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m     \u001b[39m# We cannot return the gradient norm because DeepSpeed does it.\u001b[39;00m\n\u001b[1;32m   2100\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munscale_gradients()\n\u001b[1;32m   2102\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[39m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/accelerate/accelerator.py:2064\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2062\u001b[0m     gradients \u001b[39m=\u001b[39m xm\u001b[39m.\u001b[39m_fetch_gradients(opt)\n\u001b[1;32m   2063\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduce(gradients, scale\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_processes)\n\u001b[0;32m-> 2064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39munscale_(opt)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub-3.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
