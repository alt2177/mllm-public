Metadata-Version: 2.1
Name: mllm
Version: 0.1.0
Summary: A package for merging deep neural networks, including large language models
Author-email: Robert Thompson <robert_thompson@berkeley.edu>, "Phudish (Tam) Prateepamornkul" <phudish_p@berkeley.edu>, Sean McAvoy <sean_mcavoy@berkeley.edu>, Austin Tao <austin.tao@berkeley.edu>
License: MIT
Project-URL: repository, https://github.com/alt2177/Merged-LLMs
Keywords: merging,deep neural networks,LLMs
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE

# Merged-LLMs (MLLM)

A framework for evaluating different merged, pre-trained LLMs

## Installation Instructions

All of our dependencies are specified in the `requirements.txt` file. To run any of
our experiments, you will want to create a virtual enviromnent and install dependencies.
If you are on a remote machine that uses `slurm` to schedule and run jobs, you can simply
run

```
sbatch bin/run_experiment.sh
```

which will take care of all dependency installations to execute an example script.
Otherwise, run the following to set up a local venv:

```{bash}
python -m venv mllm
source mllm/bin/activate
pip install -r ./requirements.txt
```

## Usage

If you wish to run your job using `slurm`, run the following command from the top-level
directory:

```{bash}
sbatch bin/{NAME_OF_YOUR_SBATCH_FILE}.sh
```

Note that if you try to run this command while in the `bin` directory, you will have to
make sure your `.sh` script takes that into account and adjusts your `pwd` as necessary.

As an example, `sbatch bin/run_experiment.sh` will run the following command and output
its results to `bin/outputs`, as defined in the header:

```{bash}
## Resource Requests
#SBATCH --output=./bin/outputs/output-%j.log

...

python -m src.eval_scripts.eval_gpt_linear_merge
```

Simply change the module to be used in the `.sh` script for your experiment.

## Creating New Experiments

Experiments are set up through config files in `src/configs`. To define your own, create a new
`config.py` file in `src/configs`.

```{yaml}
# An example config file

config:
  dataset:
    name: "imdb"
  model:
    name: "openai-community/gpt2"
    num_labels: 2
  training_args:
    output_dir: "imdb_finetune_gpt2_test_epoch_1_with_prob_1_merge_output"
    learning_rate: 2e-5
    per_device_train_batch_size: 8
    per_device_eval_batch_size: 8
    num_train_epochs: 1
    weight_decay: 0.01
    evaluation_strategy: "epoch"
    save_strategy: "epoch"
    load_best_model_at_end: true
    push_to_hub: true
    hub_model_id: "imdb_finetune_gpt2_test_epoch_1_with_prob_1_merge_output"
  mergekit_config:
    models:
      - model: gpt2_f_experiment_0_drug_data
        parameters:
  # any other specifications you want

```

Once you have defined your new config, create a new module in `src/` and specify your new
config path:

```{python}
# Example: for eval_gpt2_linear_merge.py in src/eval_scripts/

CONFIG_YML = CONFIG_DIR_PATH / 'example_config.yml'     # change this to be your config file

# load your config
config = load_yaml_config(CONFIG_YML, 'r')


do_stuff():
    stuff(config["training_args"])

if __name__ == "__main__":
    do_stuff()

```

Afterwards, you can run your experiment with the `.sh` script as explained above.

## Citation
