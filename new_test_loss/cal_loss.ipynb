{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class label_smooth_loss(torch.nn.Module):\n",
    "    def __init__(self, num_classes, smoothing=0.1):\n",
    "        super(label_smooth_loss, self).__init__()\n",
    "        eps = smoothing / num_classes\n",
    "        self.negative = eps\n",
    "        self.positive = (1 - smoothing) + eps\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=1)\n",
    "        true_dist = torch.zeros_like(pred)\n",
    "        true_dist.fill_(self.negative)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.positive)\n",
    "        return torch.sum(-true_dist * pred, dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_loss = label_smooth_loss(10)\n",
    "data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_ties_test_probabilities.csv\")\n",
    "data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_1_drug_data_test_probabilities.csv\")\n",
    "data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1543, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "log_probs = torch.log(data_pred)\n",
    "value = loss(log_probs, data_true_label)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_loss = label_smooth_loss(10)\n",
    "data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_ties_test_probabilities.csv\")\n",
    "data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1543, dtype=torch.float64)\n",
      "tensor(7.1735)\n",
      "tensor(2.6562, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "log_probs = -torch.log(data_pred)\n",
    "if data_true_label.dim() == log_probs.dim() - 1:\n",
    "    labels = data_true_label.unsqueeze(-1)\n",
    "padding_mask = labels.eq(-100)\n",
    "labels = torch.clamp(labels, min=0)\n",
    "nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n",
    "nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "\n",
    "# Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\n",
    "num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "nll_loss = nll_loss.sum() / num_active_elements\n",
    "smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "print(nll_loss)\n",
    "print(smoothed_loss)\n",
    "total_loss = (1 - 0.1) * nll_loss + 0.1 * smoothed_loss\n",
    "print(total_loss)\n",
    "# padding_mask = labels.eq(self.ignore_index)\n",
    "# # In case the ignore_index is -100, the gather will fail, so we replace labels by 0. The padding_mask\n",
    "# # will ignore them in any case.\n",
    "# labels = torch.clamp(labels, min=0)\n",
    "# nll_loss = log_probs.gather(dim=-1, index=labels)\n",
    "# # works for fp16 input tensor too, by internally upcasting it to fp32\n",
    "# smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)\n",
    "\n",
    "# nll_loss.masked_fill_(padding_mask, 0.0)\n",
    "# smoothed_loss.masked_fill_(padding_mask, 0.0)\n",
    "\n",
    "# # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):\n",
    "# num_active_elements = padding_mask.numel() - padding_mask.long().sum()\n",
    "# nll_loss = nll_loss.sum() / num_active_elements\n",
    "# smoothed_loss = smoothed_loss.sum() / (num_active_elements * log_probs.shape[-1])\n",
    "# return (1 - self.epsilon) * nll_loss + self.epsilon * smoothed_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_pred : torch.Size([43013, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_loss = label_smooth_loss(10)\n",
    "data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_ties_test_probabilities.csv\")\n",
    "data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = loss(data_pred, data_true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0189, dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions,labels):\n",
    "    predictions = torch.argmax(predictions,dim=1)\n",
    "    correct = (predictions == labels).sum().item()\n",
    "    accuracy = correct/len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(predictions,labels):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    return loss(predictions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_yelp_test_finetune_accuracy.txt\",\"w\") as f:\n",
    "    for i in range(10):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/yelp_experiment_probs/gpt2_f_experiment_{i}_test_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "        f.write(f'Finetune test accuracy {i}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_yelp_val_finetune_accuracy.txt\",\"w\") as f:\n",
    "    for i in range(10):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/yelp_experiment_probs/gpt2_f_experiment_{i}_validation_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "        f.write(f'Finetune validaiton accuracy {i}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_yelp_test_finetune_loss.txt\",\"w\") as f:\n",
    "    for i in range(10):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/yelp_experiment_probs/gpt2_f_experiment_{i}_test_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        loss = calculate_loss(data_pred,data_true_label)\n",
    "        f.write(f'Finetune test loss {i}: {loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_yelp_val_finetune_loss.txt\",\"w\") as f:\n",
    "    for i in range(10):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/yelp_experiment_probs/gpt2_f_experiment_{i}_validation_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        loss = calculate_loss(data_pred,data_true_label)\n",
    "        f.write(f'Finetune validation loss\" {i}: {loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_test_finetune_accuracy.txt\",\"w\") as f:\n",
    "    for i in range(5):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_{i}_drug_data_test_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "        f.write(f'Finetune test accuracy {i}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_val_finetune_accuracy.txt\",\"w\") as f:\n",
    "    for i in range(5):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_{i}_drug_data_validation_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "        f.write(f'Finetune validation accuracy {i}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_test_finetune_loss.txt\",\"w\") as f:\n",
    "    for i in range(5):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_{i}_drug_data_test_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        loss = calculate_loss(data_pred,data_true_label)\n",
    "        f.write(f'Finetune test loss {i}: {loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_val_finetune_loss.txt\",\"w\") as f:\n",
    "    for i in range(5):\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_{i}_drug_data_validation_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        loss = calculate_loss(data_pred,data_true_label)\n",
    "        f.write(f'Finetune validation accuracy {i}: {loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_xl_test_accuracy.txt\",\"w\") as f:\n",
    "    data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_drug_data_large_test_probabilities.csv\")\n",
    "    data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "    data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "    accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "    f.write(f'Finetune test accuracy big : {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_xl_val_accuracy.txt\",\"w\") as f:\n",
    "    data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_xl_validation_probabilities.csv\")\n",
    "    data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "    data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "    accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "    f.write(f'Finetune test accuracy big : {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = [\"dare_linear\",\"dare_ties\",\"linear\",\"ties\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_test_merge_method_accuracy.txt\",\"w\") as f:\n",
    "    for merge_type in merge:\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_{merge_type}_test_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "        f.write(f'Merge method test accuracy {merge_type}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_test_merge_method_loss.txt\",\"w\") as f:\n",
    "    for merge_type in merge:\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_{merge_type}_test_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        loss = calculate_loss(data_pred,data_true_label)\n",
    "        f.write(f'Merge method test accuracy {merge_type}: {loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_val_merge_method_accuracy.txt\",\"w\") as f:\n",
    "    for merge_type in merge:\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_{merge_type}_validation_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "        f.write(f'Merge method test accuracy {merge_type}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"accuracy_manual_results_drug_val_merge_method_loss.txt\",\"w\") as f:\n",
    "    for merge_type in merge:\n",
    "        data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_m_experiment_drug_data_{merge_type}_validation_probabilities.csv\")\n",
    "        data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "        data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "        loss = calculate_loss(data_pred,data_true_label)\n",
    "        f.write(f'Merge method test accuracy {merge_type}: {loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    data = pd.read_csv(f\"mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_{i}_drug_data_test_probabilities.csv\")\n",
    "    data_true_label = torch.from_numpy(data['true_label'].values)\n",
    "    data_pred = torch.from_numpy(data.drop(columns=['true_label']).values)\n",
    "    torch.save(data_pred,f'mllm/new_test_loss/test_loss_exp/drug_experiment_probs/gpt2_f_experiment_{i}_drug_data_test_probabilities.pt')\n",
    "    #accuracy = calculate_accuracy(data_pred,data_true_label)\n",
    "    #f.write(f'Finetune test accuracy {i}: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
